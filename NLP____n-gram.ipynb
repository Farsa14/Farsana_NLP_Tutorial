{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP  n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is n-gram\n",
    "\n",
    "    used in text mining and NLP \n",
    "    continuos sequence of N tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use n-gram\n",
    "\n",
    "Lets start with Character level\n",
    "\n",
    "Sentence = ['I am good']\n",
    "\n",
    "1-gram/unigram = [i, ,a,m, ,g,o,o,d]\n",
    "2-gram/bigram  = ['i ',' a','am','m ',' g','go','oo','od']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def ngram(text,n):\n",
    "    token = re.split(\"\\\\s+\", text)\n",
    "    ngrams=[]\n",
    "    for i in range(len(token)-n+1):\n",
    "        temp =[token[j]for j in range(i,i+n)]\n",
    "        ngrams.append(\" \".join(temp))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movie is fantastically bad, still',\n",
       " 'is fantastically bad, still had',\n",
       " 'fantastically bad, still had a',\n",
       " 'bad, still had a good',\n",
       " 'still had a good time',\n",
       " 'had a good time with',\n",
       " 'a good time with my',\n",
       " 'good time with my friends']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Movie is fantastically bad, still had a good time with my friends\"\n",
    "ngram(text,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram appication\n",
    "\n",
    " 1. Spelling Correction\n",
    "   \n",
    "  \"wow\" => \"wo\" \"ow\" => input = woo ==> wow,\n",
    "  \n",
    " 2. Next word prediction\n",
    "  \n",
    "  S1 = How r u \n",
    "  s2 = r u good \n",
    "  s3 = r u there \n",
    "  s4 = How r u doing  \n",
    "  \n",
    "  How r u - 2\n",
    "  r u good - 1 \n",
    "  r u there - 1\n",
    "  r u doing - 1\n",
    "  \n",
    "  whats our next word in below phrase.....u/good/there/doing??????\n",
    "  \n",
    "  How r .....UUU\n",
    "  \n",
    "      \n",
    " 3. Word breaking\n",
    "  \n",
    " 4. text summarization\n",
    "  .......\n",
    "  \n",
    "  In our next tutorial, we will train the model by implemeting the logics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps towards word2vec\n",
    "  \n",
    "  Baics of DeepLearning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
